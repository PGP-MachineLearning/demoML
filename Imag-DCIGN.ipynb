{"cells":[{"cell_type":"markdown","metadata":{"id":"r6X3sZdb3I7T"},"source":["# Demo de Deep Convolutional Inverse Graphics Network (DCIGN) para colorear las imágenes"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"NafdiEwq3IMh"},"outputs":[],"source":["#@title Librerías a usar\n","from tensorflow import keras\n","from keras.models import Model\n","from tensorflow.keras.utils import plot_model\n","from keras.utils import np_utils\n","\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","import random\n","import copy\n","\n","import ipywidgets as widgets\n","from ipywidgets import Box, Layout\n","\n","from PIL import Image\n","\n","print(\"\\nLibrerías importadas\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0pkkaUToGKc7"},"outputs":[],"source":["#@title Acceder al Drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# directorio local en Google Drive\n","path = 'gdrive/My Drive/IA/demoML/imagenes/ANIMALES' #@param {type:\"string\"}\n","path_entrenamiento = '/train'  #@param {type:\"string\"}\n","path_prueba = '/test'  #@param {type:\"string\"}\n","\n","imagPath_train = path + path_entrenamiento\n","imagPath_test = path + path_prueba"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GE_3cMSTyzfG"},"outputs":[],"source":["#@title Cargar Imágenes usando en blanco y negro para, y color para salida\n","\n","#@markdown ### Parámetros para imágenes:\n","imagen_ancho = 128 #@param {type:\"integer\"}\n","imagen_largo = 128 #@param {type:\"integer\"}\n","\n","# tamaño de las imágenes\n","if imagen_ancho<=10:\n","  imagen_largo = 10\n","if imagen_largo<=10:\n","  imagen_largo = 10\n","IMAGE_SHAPE = (imagen_ancho, imagen_largo, 3)\n","\n","# indica si se usan las imágenes generadas por data augmentation\n","usarDA = False\n","\n","# define función para cargar las imágenes\n","def cargarImagenes(imagPath):\n","  classes_ori = [] \n","  images_ori = []\n","  esDA_ori = []\n","  images_sal = []\n","\n","  all_dirs = os.listdir( imagPath )\n","  for each_dir in all_dirs:\n","\n","      auxiPath = imagPath + '/' + each_dir \n","      imagFN  = os.listdir( auxiPath )\n","      for each_imagFN in imagFN:\n","\n","            esImagDA = (each_imagFN[:2] == 'da')\n","                  \n","            # abre la imagen\n","            imag = Image.open(auxiPath + \"/\" + each_imagFN)\n","            \n","            # ajusta el tamaño     \n","            tipoImage = 'RGB'\n","            imag = imag.convert('RGB')\n","            imag = imag.resize((IMAGE_SHAPE[0], IMAGE_SHAPE[1]), Image.ANTIALIAS)          \n","            \n","            imag2 = imag.convert('L')\n","            imag2 = imag2.convert('RGB')\n","\n","            # transforma a un vector de nros\n","            arImag = np.array(imag)\n","            arImag2 = np.array(imag2)\n","            \n","            # agrega a los vectores\n","            classes_ori.append( each_dir )\n","            images_ori.append( arImag2 )\n","            images_sal.append( arImag )\n","            esDA_ori.append( esImagDA )\n","\n","  return classes_ori, images_ori, images_sal, esDA_ori, tipoImage\n","\n","# carga las imagenes de entrenamiento\n","classes_train, images_train_IN, images_train_OUT, esDAimag_train, tipoImage_train = cargarImagenes(imagPath_train)\n","print(\"> Para Entrenamiento: \")\n","print(\"- Clases cargadas: \", len(classes_train))\n","print(\"- Imágenes cargadas: \", len(images_train_IN))\n","\n","if len(classes_train)>0:\n","  print(\"- Ejemplo \", classes_train[0], \" \", images_train_IN[0].shape, \" \", images_train_OUT[0].shape,  \": \")\n","  display( Image.fromarray(images_train_IN[0], tipoImage_train) )\n","  display( Image.fromarray(images_train_OUT[0], tipoImage_train) )\n","\n","# carga las imagenes de prueba\n","classes_test, images_test_IN, images_test_OUT, esDAimag_test, tipoImage_test = cargarImagenes(imagPath_test)\n","print(\"\\n\\n> Para Prueba: \")\n","print(\"- Clases cargadas: \", len(classes_test))\n","print(\"- Imágenes cargadas: \", len(images_test_IN))\n","\n","if len(classes_test)>0:\n","  print(\"- Ejemplo \", classes_test[0], \" \", images_test_IN[0].shape, \" \", images_test_OUT[0].shape, \": \")\n","  display( Image.fromarray(images_test_IN[0], tipoImage_test) )\n","  display( Image.fromarray(images_test_OUT[0], tipoImage_test) )"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"eFpF2PNkSbjK"},"outputs":[],"source":["#@title Preparar imágenes para usar en el modelo\n","\n","# define función auxiliar para mostrar imágenes preparadas\n","def plot_image(imag):\n","  if IMAGE_SHAPE[2]==1:\n","    plt.imshow((imag*255).reshape(IMAGE_SHAPE[0], IMAGE_SHAPE[1]).astype(np.uint8)) \n","    plt.gray()\n","  else:\n","    plt.imshow((imag*255).reshape(IMAGE_SHAPE).astype(np.uint8)) \n","  plt.axis(\"off\")  \n","\n","def plot_images(clase, imag_IN, imag_OUT, imag_MODELO=None):\n","    # prepara para mostrar\n","    fig = plt.figure()\n","    fig.suptitle( \"clase: \" + str( clase ) )\n","\n","    # muestra la real de entrada\n","    ax1 = fig.add_subplot(121)\n","    plot_image( imag_IN )\n","\n","    ax3 = fig.add_subplot(122)\n","    if not(imag_MODELO is None):\n","      # muestra la generada por el modelo\n","      plot_image( imag_MODELO )\n","      # muestra la real deseada\n","      ax2 = fig.add_subplot(331)\n","\n","    plot_image( imag_OUT )\n"," \n","    #plt.tight_layout()\n","    fig = plt.gcf()\n","\n","# define función auxiliar para preparar la lista de imágenes a procesar\n","def prepare_imageList(imagList):    \n","  auxiAr = np.array(imagList).astype('float32') / 255.\n","  auxiAr = auxiAr.reshape((len(auxiAr), IMAGE_SHAPE[0], IMAGE_SHAPE[1], IMAGE_SHAPE[2]))  \n","  return auxiAr\n","  \n","# define función auxiliar para preparar lista de clases \n","def prepare_clasesList(classesList, dictMapeo=None):\n","  if dictMapeo==None:\n","    # genera diccionario de mapeo\n","    auxDict = list(set(classesList))\n","    dictMapeo = dict( zip( auxDict, range(len(auxDict)) ) )\n","  # realiza el mapeo\n","  y = []\n","  for cl in classesList:\n","      y.append( dictMapeo[cl] )\n","  # convierte valores numéricos a columnas de vakores binarios (i.e. one hot encoded)\n","  dummy_y = np_utils.to_categorical(y)\n","  # devuelve\n","  return np.array(y), np.array(dummy_y), dictMapeo\n","\n","# define vector auxiliar de datos de entrada para usar en el entrenamiento y prueba\n","x_train_IN = prepare_imageList(images_train_IN)\n","x_train_OUT = prepare_imageList(images_train_OUT)\n","x_test_IN = prepare_imageList(images_test_IN)\n","x_test_OUT = prepare_imageList(images_test_OUT)\n","\n","# define vector auxiliar de datos de salida para usar en el entrenamiento y prueba\n","# también usa esta información para determinar la cantida de neuronas de salida\n","y_train, y_trainEnc, dictMapeo = prepare_clasesList(classes_train)\n","y_test, y_testEnc,_ = prepare_clasesList(classes_test, dictMapeo)\n","\n","# genera diccionario auxiliar para poder convertir de ID de clase a nombre de clase\n","CLASES = [ x for x,y in dictMapeo.items() ]\n","\n","print(\"> Para Entrenamiento: \")\n","print(\" - x_train (cant ejemplos, datos entrada): \", x_train_IN.shape, \"/\",  x_train_OUT.shape)\n","print(\" - y_trainEnc (cant): \", len(y_trainEnc))\n","print(\" - y_train (cant): \", len(y_train))\n","print(\"\\n\\n> Para Prueba: \")\n","print(\" - x_test (cant ejemplos, datos entrada): \", x_train_IN.shape, \"/\",  x_train_OUT.shape)\n","print(\" - y_testEnc (cant): \", len(y_testEnc))\n","print(\" - y_test (cant): \", len(y_test))\n","print(\"\\n\\n> Para Ambos: \")\n","print(\" - dictMapeo: \", dictMapeo)\n","print(\" - CLASES: \", CLASES)\n","if len(y_train)>0:\n","  print(\"\\n - Imagen ejemplo de \", CLASES[y_train[0]],  \"(\", y_train[0], \" / \", y_trainEnc[0], \")\")\n","  plot_images(CLASES[y_train[0]], x_train_IN[0], x_train_OUT[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"LEwtvKsnJLBN"},"outputs":[],"source":["#@title Establecer modelo\n","\n","#@markdown ### Parámetros de las capas ConvNet:\n","convNet_tamaño_kernel_NxN = 3 #@param {type:\"integer\"}\n","convNet_tamaño_pooling_MxM =  2#@param {type:\"integer\"}\n","convNet_cantidad_capas_ocultas =  7#@param {type:\"integer\"}\n","convNet_tipo_capas = 'LeakyReLU' #@param [ 'Conv2D + UpSampling', 'Conv2DTranspose + BatchNormalization', 'LeakyReLU']\n","\n","#@markdown ### Parámetros del Optimizador:\n","opt_tipo = \"Adam\" #@param [\"Gradiente Decreciente\", \"Adam\", \"Adadelta\", \"Adagrad\", \"Adamax\", \"Nadam\", \"FTRL\"]\n","opt_learning_rate = 0.001 #@param {type: \"number\"}\n","\n","## aplicación de los parámetros elegidos\n","\n","# tamaño de los kernels y pooling (para simplificar son todas iguales)\n","if convNet_tamaño_kernel_NxN<1:\n","  convNet_tamaño_kernel_NxN = 1\n","dae_kernel_shape = (convNet_tamaño_kernel_NxN, convNet_tamaño_kernel_NxN)\n","if convNet_tamaño_pooling_MxM<1:\n","  convNet_tamaño_pooling_MxM=1\n","dae_pooling_shape = (convNet_tamaño_pooling_MxM, convNet_tamaño_pooling_MxM)\n","\n","# indica la configuración para la parte Encoder \n","#   (cada elemento de las listas son la configuración de las capas Conv)\n","if convNet_cantidad_capas_ocultas<1:\n","  convNet_cantidad_capas_ocultas = 1\n","dae_filters = []\n","for i in range(convNet_cantidad_capas_ocultas, 0, -1):\n","  dae_filters.append( 2**(i+2) )\n","\n","# la capa de features se define automáticamente \n","# en base a la información de la última capa del Encoder\n","dae_filters.append( 'f' ) \n","dae_posLayFeat = len(dae_filters)-1\n","\n","# cantidad de neuronas ocultas para la parte Decoder \n","#   (usa la la lista de Encoder inversa)\n","for eachEncFilter in dae_filters[0:len(dae_filters)-1][::-1]:\n","      dae_filters.append( eachEncFilter )\n","\n","# indica si el Decoder usa:\n","#  ver 1: Conv2D + UpSampling (mejores colores pero fuera de foco)\n","#  ver 2: Conv2DTranspose + BatchNormalization (mejor nitidez pero como manchada)\n","if convNet_tipo_capas == 'Conv2D + UpSampling':\n","  verCapasDecoder = 1\n","elif convNet_tipo_capas == 'LeakyReLU':\n","  verCapasDecoder = 3\n","else:\n","  verCapasDecoder = 2\n","\n","  \n","if opt_tipo == \"Gradiente Decreciente\":\n","  opt = keras.optimizers.SGD(learning_rate=opt_learning_rate)\n","elif opt_tipo == \"Adam\":\n","  opt = keras.optimizers.Adam(learning_rate=opt_learning_rate)\n","elif opt_tipo == \"Adadelta\":\n","  opt = keras.optimizers.Adadelta(learning_rate=opt_learning_rate)\n","elif opt_tipo == \"Adagrad\":\n","  opt = keras.optimizers.Adagrad(learning_rate=opt_learning_rate)\n","elif opt_tipo == \"Adamax\":\n","  opt = keras.optimizers.Adamax(learning_rate=opt_learning_rate)\n","elif opt_tipo == \"Nadam\":\n","  opt = keras.optimizers.Nadam(learning_rate=opt_learning_rate)\n","elif opt_tipo == \"FTRL\":\n","  opt = keras.optimizers.Ftrl(learning_rate=opt_learning_rate)\n","else:\n","  opt = keras.optimizers.Adam()\n","\n","\n","# define la arquitectura de capas del Deep Autoencoder CNN\n","# teniendo en cuenta la definición dada anteriomente\n","input_img_Lay = tf.keras.layers.Input(shape=IMAGE_SHAPE, name='input_img') # capa de entrada\n","eachLay = input_img_Lay\n","auxName = 'enc_'\n","auxId = 1 \n","for i in range(len(dae_filters)):  \n","\n","    # define el nombre de la capa oculta\n","    auxlayerName = auxName+str(auxId)\n","    if i==dae_posLayFeat:\n","        auxlayerName = 'features'\n","        auxName = 'dec_'        \n","    else:        \n","        if auxName == 'enc_':\n","          auxId = auxId + 1\n","        else:\n","          auxId = auxId - 1\n","\n","    # agrega las capas ocultas\n","    if auxlayerName.startswith('enc_'):\n","        if verCapasDecoder == 3:\n","          # agrega capa Conv2D + LeakyReLU para Encoder\n","          eachLay = tf.keras.layers.Conv2D(dae_filters[i], dae_kernel_shape, padding='same', name='c_'+auxlayerName)(eachLay) \n","          eachLay = tf.keras.layers.LeakyReLU(alpha=0.2, name='cl_'+auxlayerName)(eachLay) \n","        else:\n","          # agrega capa Conv2D para Encoder\n","          eachLay = tf.keras.layers.Conv2D(dae_filters[i], dae_kernel_shape, activation='relu', padding='same', name='c_'+auxlayerName)(eachLay) \n","        # agrega capa MaxPooling para Encoder\n","        eachLay = tf.keras.layers.MaxPooling2D(dae_pooling_shape, padding='same', name='p_'+auxlayerName)(eachLay)\n","\n","    elif auxlayerName.startswith('dec_'):\n","      if verCapasDecoder == 2:\n","          # Dec v2: agrega capa Conv2DTranspose con BatchNormalization para Decoder        \n","          eachLay = tf.keras.layers.Conv2DTranspose(dae_filters[i], dae_kernel_shape, strides=2, activation='relu', padding='same', name='d_'+auxlayerName)(eachLay) \n","          eachLay = tf.keras.layers.BatchNormalization(name='p_'+auxlayerName)(eachLay)\n","      elif verCapasDecoder == 3:\n","          # Dec v3: agrega capa Conv2DTranspose con LeakyReLU y BatchNormalization para Decoder                  \n","          eachLay = tf.keras.layers.Conv2DTranspose(dae_filters[i], dae_kernel_shape, strides=2, padding='same', name='d_'+auxlayerName)(eachLay) \n","          eachLay = tf.keras.layers.LeakyReLU(alpha=0.2, name='dl_'+auxlayerName)(eachLay) \n","          eachLay = tf.keras.layers.BatchNormalization(axis=-1, name='dn_'+auxlayerName)(eachLay)\n","      else: \n","          # Dec v1: agrega capa Conv2 + UpSampling2D para Decoder        \n","          eachLay = tf.keras.layers.Conv2D(dae_filters[i], dae_kernel_shape, activation='relu', padding='same', name='d_'+auxlayerName)(eachLay) \n","          eachLay = tf.keras.layers.UpSampling2D(dae_pooling_shape, name='p_'+auxlayerName)(eachLay)\n","\n","    elif auxlayerName.startswith('features'):\n","          #  agrega capa Flatten, Dense y Reshape \n","          # para ello utiliza la información del shape de la última capa Encoder\n","          ##dae_features_shape = (int(eachLay.shape[1]), int(eachLay.shape[2]), IMAGE_SHAPE[2])\n","          dae_features_shape = (int(eachLay.shape[1]), int(eachLay.shape[2]),  int(eachLay.shape[3]))\n","          num_features = dae_features_shape[0]*dae_features_shape[1]*dae_features_shape[2]      \n","\n","          eachLay = tf.keras.layers.Flatten(name='f_'+auxlayerName)(eachLay)\n","          eachLay = tf.keras.layers.Dense(num_features, activation='sigmoid', name='d_'+auxlayerName)(eachLay)\n","          features_Lay = eachLay\n","          eachLay = tf.keras.layers.Reshape(dae_features_shape, name='r_'+auxlayerName)(eachLay)\n","\n","# agrega la capa de salida usando la cantidad de canales de la imagen como cantidad de filtros   \n","output_img_Lay = tf.keras.layers.Conv2D(IMAGE_SHAPE[2], dae_kernel_shape, activation='sigmoid', padding='same', name='output_img')(eachLay)  # capa de salida\n","\n","# controla que la cantidad de entradas y salidas sea igual\n","capas_entradas_salidas_diferentes = False\n","for in_s, out_s in zip(input_img_Lay.shape, output_img_Lay.shape):\n","  if in_s != out_s:\n","    capas_entradas_salidas_diferentes = True\n","    break\n","if capas_entradas_salidas_diferentes:\n","  raise Exception(\"La cantidad de entradas \" + str(input_img_Lay.shape)+ \" no coincide con la cantidad de salidas \" + str(output_img_Lay.shape)+ \": ajuste los parámetros para capas convNet y/o  aumente el tamaño de las imágenes!\")\n","\n","# genera el modelo Deep Autoencoder\n","DCIGNmodel = Model(input_img_Lay, output_img_Lay, name='DCIGNmodel')\n","DCIGNmodel.compile(optimizer=opt, loss='mse', metrics=['RootMeanSquaredError'])\n","\n","print(\"Modelo DAE CNN creado con \", len(DCIGNmodel.layers), \" capas:\")\n","DCIGNmodel.summary()\n","print(\"\\n\")\n","plot_model(DCIGNmodel, show_layer_names=True, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jmBH11GuJhk1"},"outputs":[],"source":["#@title Entrenar\n","\n","cant_epocas_entrenamiento =  1500 #@param {type:\"integer\"}\n","\n","# cantidad de épocas del entrenamiento\n","cantEpocas = (1 if cant_epocas_entrenamiento<1 else cant_epocas_entrenamiento)\n","\n","# lleva a cabo el entrenamiento\n","# usando los mismos datos como entrada y salida\n","history = DCIGNmodel.fit(x_train_IN, x_train_OUT,\n","                epochs = cantEpocas,\n","                shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"eo2dQNfZq8Bz"},"outputs":[],"source":["#@title Mostrar Gráficos del Entrenamiento\n","plt.figure(figsize=(15,8)) \n","plt.plot(history.history['loss'])\n","plt.title('Gráfico del Error del Entrenamiento')\n","plt.ylabel('')\n","plt.xlabel('epoch')\n","plt.show()\n","\n","plt.figure(figsize=(15,8)) \n","plt.plot(history.history['root_mean_squared_error'])\n","plt.title('Gráfico de la Distancia Media Cuadrática Mínima del Entrenamiento')\n","plt.ylabel('')\n","plt.xlabel('epoch')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfRff6b-LOxG","cellView":"form"},"outputs":[],"source":["#@title Evaluar modelo con imágenes de Entrenamiento\n","\n","# para sacar warning por cantidad de imágenes mostradas\n","plt.rcParams.update({'figure.max_open_warning': 0})\n","\n","def agregarRuidoImagen(x_datos, gradoRuido):\n","  # copia datos para no cambiar original\n","  x_datos = copy.deepcopy(x_datos)\n","  # si es negativo, determina el grado al azar\n","  if gradoRuido < 0:\n","    gradoRuido = random.randint(0, 100)\n","  # agrega ruido al azar  \n","  x_datos = x_datos + np.random.normal(loc=0.0, scale=gradoRuido/100, size=x_datos.shape)\n","  x_datos = np.clip(x_datos, 0., 1.)\n","  return x_datos\n","\n","\n","# función auxiliar para probar el modelo entrenado en detalle\n","def probarModelo(x_in, x_out, y, esDAimag, claseFiltrar=None, gradoRuido=0.0):\n","\n","  # determina clase a filtrar\n","  if (claseFiltrar is None) or (claseFiltrar == \"TODOS\"):\n","        clFiltrarID = None\n","  else:\n","        clFiltrarID = dictMapeo[claseFiltrar]\n","\n","  # agrega ruido según corresponda\n","  if gradoRuido != 0.0:\n","    x_in = agregarRuidoImagen(x_in, gradoRuido)\n","\n","    # procesa las imágenes con el modelo \n","  reconstr_imgs = DCIGNmodel.predict(x_in, verbose=0)\n","  cantMostradas = 0\n","  # muestra las 15 primeras imágenes \n","  for i in range(len(x_in)):\n","    # determina si filtra por clase\n","    if (clFiltrarID is None) or (clFiltrarID == y[i]):    \n","    # no muestra las generadas por DA\n","    #if not esDAimag[i]:\n","          # prepara para mostrar\n","          plot_images(CLASES[y[i]], x_in[i], x_out[i], reconstr_imgs[i])\n","          cantMostradas = cantMostradas + 1\n","  print(\"\\n\\t Cantidad imágenes mostradas: \", cantMostradas, \"\\n\")\n","\n","# genera toda la interface para evaluar modeo DAE\n","def crearUI_evaluarModeloDAE(clDefecto, ruidoPorDefecto, funcionCambiaSeleccion):\n","  # prepara combo para filtrar por clase\n","  seleccion_CLASES = [\"-\", \"TODOS\"]\n","  seleccion_CLASES.extend( CLASES )\n","  seleccion_CLASES.sort()\n","\n","  # auxiliar para que muestre bien la descripción\n","  style_3D = {'description_width': 'initial'}\n","\n","  combo_clase = widgets.Dropdown(\n","      options = seleccion_CLASES,\n","      value = clDefecto,\n","      description = 'Filtrar por clase:',\n","      style=style_3D,\n","      disabled = False,\n","  )\n","\n","  # prepara para seleccionar grado de ruido\n","  gradoRuidoImagenes = widgets.FloatSlider(\n","          value=ruidoPorDefecto,\n","          min=-1,\n","          max=100,\n","          step=1,\n","          description='Grado ruido para agregar:',\n","          style=style_3D,\n","          disabled=False,\n","          continuous_update=False,\n","          orientation='horizontal',\n","          readout=True,\n","          readout_format='.1f',)\n","\n","\n","  prueba_ui = widgets.GridBox(children=[combo_clase, gradoRuidoImagenes],\n","          layout=Layout(width='100%') \n","        )\n","  out_prueba = widgets.interactive_output(funcionCambiaSeleccion, {'cl':combo_clase, 'r':gradoRuidoImagenes})\n","\n","  return prueba_ui, out_prueba\n","\n","# función para filtrar por clase\n","def cambiaSeleccion_clase_evaluar_imEntrenamiento(cl, r):  \n","  if (cl == \"-\"):\n","    return\n","    \n","  # prueba con los datos de entrenamiento\n","  print(\"*** Resultados con datos de Entrenamiento: clase \"+cl)\n","  probarModelo(x_train_IN, x_train_OUT, y_train, esDAimag_train, cl, r)  \n","\n","# muestra la interface \n","ev_entrenamiento_ui, ev_entrenamiento_out = crearUI_evaluarModeloDAE( \"-\", 0.0, cambiaSeleccion_clase_evaluar_imEntrenamiento )\n","display(ev_entrenamiento_ui, ev_entrenamiento_out)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTha04q2Hlsx","cellView":"form"},"outputs":[],"source":["#@title Evaluar el modelo con las imágenes de prueba\n","\n","# función para filtrar por clase\n","def cambiaSeleccion_clase_evaluar_imPrueba(cl, r):  \n","  if (cl == \"-\"):\n","    return\n","  # prueba con los datos de entrenamiento\n","  print(\"*** Resultados con datos de Prueba: clase \"+cl)\n","  probarModelo(x_test_IN, x_test_OUT, y_test, esDAimag_test, cl, r)\n","\n","# muestra la interface \n","ev_prueba_ui, ev_prueba_out = crearUI_evaluarModeloDAE( \"-\", 0.0, cambiaSeleccion_clase_evaluar_imPrueba )\n","display(ev_prueba_ui, ev_prueba_out)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1gEcMqfeYOJnjxHE-AACF0t8SJTXIsif6","timestamp":1580735169391}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}