{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Texto-generar-RNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9Uk_DNw5983j"},"source":["# Generación de Texto usando una Recurrent Neuronal Network del tipo RNN básica, LSTM o GRU\n","Basado en https://www.tensorflow.org/tutorials/text/text_generation"]},{"cell_type":"markdown","metadata":{"id":"7Mjqn0ta-JmL"},"source":["1) Cargar las librerías:"]},{"cell_type":"code","metadata":{"id":"MAgWSjOw-JwI","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1626869061763,"user_tz":180,"elapsed":2359,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"214ee5c3-2ba4-4cc8-f000-bcc20b3048ef"},"source":["#@title Librerías a usar\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import csv\n","\n","print(\"Librerías cargadas\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Librerías cargadas\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"form","id":"Sb3btPP-lC4U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626869061764,"user_tz":180,"elapsed":17,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"5494a7f9-7e08-44aa-e145-a0873d53a4ec"},"source":["#@title Define clases auxiliares\n","\n","\n","# define la clase para el modelo\n","class RNNCustomModel(tf.keras.Model):\n","  def __init__(self, capa_oculta_tipo, vocab_size, embedding_dim, rnn_units):\n","    super().__init__(self, name=\"GeneradorTexto\")\n","    # datos de config\n","    self.tipoModelo = capa_oculta_tipo\n","    self.vocab_size = vocab_size\n","    self.embedding_dim = embedding_dim\n","    self.rnn_units = rnn_units\n","    # capa de entrada\n","    if (self.tipoModelo == 'LSTM') or (self.tipoModelo == 'GRU'): \n","      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"entrada\")\n","    else:\n","      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                  batch_input_shape=[1, None], name=\"entrada\")\n","    # capa oculta    \n","    if self.tipoModelo == 'LSTM': \n","      self.hdd = tf.keras.layers.LSTM(rnn_units,\n","                                    return_sequences=True,\n","                                    return_state=True,\n","                                    name=\"oculta\")\n","\n","    elif self.tipoModelo == 'GRU': \n","      self.hdd = tf.keras.layers.GRU(rnn_units,\n","                                    return_sequences=True,\n","                                    return_state=True,\n","                                    name=\"oculta\")\n","    else:\n","        self.hdd = tf.keras.layers.SimpleRNN(rnn_units,\n","                      return_sequences=True,\n","                      stateful=True,\n","                      recurrent_initializer='glorot_uniform',\n","                      name=\"oculta\")\n","    # capa de salida\n","    self.dense = tf.keras.layers.Dense(vocab_size, name=\"salida\")\n","\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","    x = self.embedding(x, training=training)\n","    if states is None:\n","      states = self.hdd.get_initial_state(x)\n","    if self.tipoModelo == 'GRU': \n","      x, states = self.hdd(x, initial_state=states, training=training)\n","    else:\n","      states = self.hdd(x, initial_state=states, training=training)\n","    x = self.dense(x, training=training)\n","\n","    if return_state:\n","      return x, states\n","    else:\n","      return x\n","\n","# clases para generar texto\n","class GeneradorTexto:\n","\n","  def __init__(self, model=None, char2idx=None, idx2char=None, caracterJoin=None):\n","    self.model = model\n","    self.char2idx = char2idx\n","    self.idx2char = idx2char\n","    self.caracterJoin = caracterJoin\n","\n","  # define función auxiliar para devolver predicción de texto\n","  def generar(self, temperature=0.1, texto_inicial=' ', cant_generar=100):\n","\n","    # Converting our start string to numbers (vectorizing)\n","    if self.caracterJoin == '':\n","      aux_input = texto_inicial\n","    else:\n","      aux_input = texto_inicial.split(self.caracterJoin)\n","    input_eval = [self.char2idx[s] for s in aux_input]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    # Empty string to store our results\n","    text_generated = []\n","\n","    # Here batch size == 1\n","    self.model.reset_states()\n","    for i in range(cant_generar):\n","        predictions = self.model(input_eval)\n","        # remove the batch dimension\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # using a categorical distribution to predict the word returned by the model\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        # We pass the predicted word as the next input to the model\n","        # along with the previous hidden state\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        text_generated.append(self.idx2char[predicted_id])\n","\n","    return (texto_inicial + self.caracterJoin.join(text_generated))\n","\n","  # define función para grabar el modelo con toda la información asociada\n","  def grabar(self, dir):    \n","    print(\"\\n\")\n","    # crea el directorio\n","    if not os.path.exists(dir):\n","      os.mkdir(dir)\n","    # exporta los pesos\n","    pesosAr = dir + '/pesos'\n","    self.model.save_weights(pesosAr, save_format='tf')\n","    print(\"Pesos del modelo grabados en \", dir)    \n","    datosAr = dir + '/configModelo.csv'\n","    # exporta los datos\n","    with open(datosAr, mode='w') as csvfile:\n","      wr = csv.writer(csvfile, delimiter=',')\n","      # para model\n","      wr.writerow([self.model.tipoModelo])\n","      wr.writerow([self.model.vocab_size])\n","      wr.writerow([self.model.embedding_dim])\n","      wr.writerow([self.model.rnn_units])\n","      # para generar texto\n","      wr.writerow(self.idx2char) \n","      wr.writerow(self.caracterJoin)\n","    print('Datos asociados al modelo grabados en' + datosAr)\n","    print(\"\\n\")\n","    return self\n","\n","  # define función para cargar un modelo con toda la información asociada\n","  def cargar(self, dir):\n","    print(\"\\n\")\n","    # controla que el directorio exista\n","    if not os.path.exists(dir):\n","      print(\"No existe el directorio a cargar!\")\n","      return None\n","    # carga datos de configuración\n","    datosAr = dir + '/configModelo.csv'\n","    with open(datosAr, mode='r') as csvfile:\n","      r = csv.reader(csvfile, delimiter=',')\n","      # para model\n","      capa_oculta_tipo = r.__next__()[0]\n","      vocab_size = int(r.__next__()[0])\n","      embedding_dim = int(r.__next__()[0])\n","      rnn_units = int(r.__next__()[0])\n","      # para generar texto\n","      self.idx2char = r.__next__()\n","      self.char2idx = {u:i for i, u in enumerate(self.idx2char)}\n","      auxCaracterJoin = r.__next__()\n","      if len(auxCaracterJoin)==0:\n","        self.caracterJoin = ''\n","      else:\n","        self.caracterJoin = ' '\n","    print('Datos asociados al modelo cargados de' + datosAr)\n","    # crea el modelo y carga los pesos\n","    # crea el modelo\n","    self.model = RNNCustomModel(\n","        capa_oculta_tipo = capa_oculta_tipo,\n","        vocab_size=vocab_size,\n","        embedding_dim=embedding_dim,\n","        rnn_units=rnn_units)\n","    pesosAr = dir + '/pesos'\n","    self.model.load_weights(pesosAr)\n","    print(\"Pesos del modelo cargados de \", dir)    \n","    print(\"\\n\")\n","    return self\n","\n","print(\"Clases auxiliares definidas\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Clases auxiliares definidas\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gGFu1mdg-N5w"},"source":["2) Cargar el texto base a procesar:"]},{"cell_type":"code","metadata":{"id":"etDVW375lv5u","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626869079949,"user_tz":180,"elapsed":18199,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"d9bfbdf7-58e3-43be-f785-c3548297099b"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# directorio local en Google Drive\n","path = 'gdrive/My Drive/IA/demoML/texto'  #@param {type:\"string\"}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U1h1PnwK-Q1Y","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626869081876,"user_tz":180,"elapsed":1940,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"bb6a1a66-2978-4ce2-b27a-f9d9c571351e"},"source":["nombre_archivo = \"/Tolkien.txt\"  #@param {type:\"string\"}\n","\n","# levanta el archivo de texto del Drive para procesar\n","text_cargado = open(\"\".join([path, nombre_archivo]), 'rb').read().decode(encoding='utf-8', errors='ignore')\n","\n","print(\"> Archivo cargado:\")\n","print ('\\n -- Tamaño total del texto: {} caracteres'.format(len(text_cargado)))\n","\n","# muestra los primeros 250 caracteres del texto\n","print(\"\\n -- Ejemplo: \\n\", text_cargado[:250])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["> Archivo cargado:\n","\n"," -- Tamaño total del texto: 4050936 caracteres\n","\n"," -- Ejemplo: \n"," ﻿EL HOBBIT\r\n","\r\n","J.R.R. TOLKIEN\r\n","\r\n","1\r\n","UNA TERTULIA INESPERADA\r\n","\r\n","En un agujero en el suelo, vivía un hobbit. No un agujero húmedo, sucio, repugnante, con restos de gusanos y olor a fango, ni tampoco un agujero seco, desnudo y arenoso, sin nada en que se\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mIgEntMT-ZbB"},"source":["3) Preparar el texto base a procesar:"]},{"cell_type":"code","metadata":{"id":"W0NuYR750LvF","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626869081878,"user_tz":180,"elapsed":14,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"bc453ef8-b128-42c3-d6af-9c2246f173f8"},"source":["#@title Limpiar el texto\n","\n","sacar_caracteres_especiales = True #@param {type:\"boolean\"}\n","sacar_signos_puntuacion = True #@param {type:\"boolean\"}\n","sacar_otros_signos = True #@param {type:\"boolean\"}\n","sacar_acentos = True #@param {type:\"boolean\"}\n","pasar_minusculas = True #@param {type:\"boolean\"}\n","\n","# hace una copia por si se vuelve a ejecutar\n","text = str(text_cargado)\n","\n","# siempre saca símbolo de inicio\n","text = text.replace('\\ufeff', ' ')\n","\n","if sacar_caracteres_especiales:\n","  text = text.replace('\\n', ' ')\n","  text = text.replace('\\t', ' ')\n","  text = text.replace('\\r', ' ')   \n","\n","if sacar_signos_puntuacion:\n","  text = text.replace(',', ' ')\n","  text = text.replace(';', ' ')\n","  text = text.replace('.', ' ')\n","  text = text.replace('¡', ' ')\n","  text = text.replace('¿', ' ')\n","  text = text.replace('!', ' ')\n","  text = text.replace('?', ' ')  \n","\n","if sacar_otros_signos:\n","  text = text.replace('-', ' ')\n","  text = text.replace(':', ' ')\n","  text = text.replace('\\'', ' ')\n","  text = text.replace('\"', ' ')\n","  text = text.replace('“', ' ')\n","  text = text.replace('”', ' ')\n","  text = text.replace('`', ' ')\n","  text = text.replace('[', ' ')\n","  text = text.replace(']', ' ')\n","  text = text.replace('(', ' ')\n","  text = text.replace(')', ' ')\n","  text = text.replace('<', ' ')\n","  text = text.replace('>', ' ')\n","  text = text.replace('=', ' ')\n","  text = text.replace('/', ' ')\n","  text = text.replace('@', ' ')\n","  text = text.replace('~', ' ')\n","  text = text.replace('*', ' ')\n","  text = text.replace('_', ' ')\n","\n","# pasa todo a minúsculas\n","if pasar_minusculas:\n","  text = text.lower()\n","\n","# eliminar acentos (reemplaza por letra sin acento)\n","if sacar_acentos:\n","  text = text.replace('á', 'a')\n","  text = text.replace('é', 'e')\n","  text = text.replace('í', 'i')\n","  text = text.replace('ó', 'o')\n","  text = text.replace('ú', 'u')\n","  text = text.replace('Á', 'a')\n","  text = text.replace('É', 'e')\n","  text = text.replace('Í', 'i')\n","  text = text.replace('Ó', 'o')\n","  text = text.replace('Ú', 'u')\n","\n","# saca todos los doble espacios (siempre)\n","text = text.replace('  ', ' ')\n","\n","print('\\n -- Tamaño total del texto luego de la limpieza: {} caracteres'.format(len(text)))\n","print(\"\\n -- Ejemplo luego de la limpieza: \\n\", text[:250])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," -- Tamaño total del texto luego de la limpieza: 3917084 caracteres\n","\n"," -- Ejemplo luego de la limpieza: \n","  el hobbit  j r r tolkien  1 una tertulia inesperada  en un agujero en el suelo vivia un hobbit no un agujero humedo sucio repugnante con restos de gusanos y olor a fango ni tampoco un agujero seco desnudo y arenoso sin nada en que sentarse o que com\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jvaOGfdL-gnP","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1626869082209,"user_tz":180,"elapsed":340,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"fa936d81-db40-4951-dee8-73a3ac71ff9b"},"source":["#@title Preparar texto \n","tipo_datos = \"palabras\" #@param [\"caracteres\", \"palabras\"]\n","\n","if tipo_datos == \"caracteres\":\n","  # The unique characters in the file\n","  auxText = text\n","  caracterJoin = ''\n","  vocab = sorted(set(auxText))\n","  print('{} caracteres distintos detectados'.format(len(vocab)))\n","else:\n","  auxText = text.split(' ')\n","  caracterJoin = ' '\n","  vocab = sorted(set(auxText))\n","  print('{} palabras distintas detectadas'.format(len(vocab)))\n","\n","# Creating a mapping from unique characters to indices\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","text_as_int = np.array([char2idx[c] for c in auxText])\n","\n","print('\\nEjemplos de Codificación \\n{')\n","for char,_ in zip(char2idx, range(10)):\n","    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n","print('  ...\\n}')\n","\n","# Muestra ejemplo de cómo se mapean los caracteres a valores numéricos\n","print ('\\n{} <-------- > {}'.format(repr(text[:13]), text_as_int[:13]))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["30929 palabras distintas detectadas\n","\n","Ejemplos de Codificación \n","{\n","  ''  :   0,\n","  '0' :   1,\n","  '1' :   2,\n","  '10':   3,\n","  '100':   4,\n","  '101':   5,\n","  '102':   6,\n","  '103':   7,\n","  '104':   8,\n","  '105':   9,\n","  ...\n","}\n","\n","' el hobbit  j' <-------- > [    0 10788 15444     0 17034 23637 23637 28162     0     2 28993 27904\n"," 16296]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jph0s63Q-s55","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626869089118,"user_tz":180,"elapsed":6914,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"48dfb143-eac8-43b7-cee8-037cca4c44ec"},"source":["#@title Armar secuencias de texto y formatear\n","\n","# determinar el largo máximo de la secuencia\n","if ((len(text)//101)<1000):\n","  seq_length = 50\n","else:\n","  seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1)\n","print(\"Largo de secuencias: \", seq_length)\n","print(\"\\n\")\n","print(\"Ejemplos por época: \", examples_per_epoch)\n","\n","# Dividir en datos de entrenamiento y prueba, para ello divide el texto en secuencias donde \n","#- la secuencia de la posición 0 a [seq_length] se considera de entrada, y \n","#- la secuencia de la posición  [seq_length+1] al final es la de salida\n","\n","# genera un vector de caracteres\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","# procesa para generar las secuencias el largo deseado\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","# muestra ejemplo\n","for item in sequences.take(5):\n","  print(repr(caracterJoin.join(idx2char[item.numpy()])))\n","\n","# genera las secuencias de entrada y salida\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","datasetSeq = sequences.map(split_input_target)\n","\n","print(\"\\nDatasetSeq: \", datasetSeq, \"\\n\")\n","\n","# muestra ejemplo\n","for input_example, target_example in  datasetSeq.take(2):\n","  print ('Texto de Entrada: ', repr(caracterJoin.join(idx2char[input_example.numpy()])))\n","  print ('Texto  de Salida:', repr(caracterJoin.join(idx2char[target_example.numpy()])))\n","  print(\"\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Largo de secuencias:  100\n","\n","\n","Ejemplos por época:  38783\n","' el hobbit  j r r tolkien  1 una tertulia inesperada  en un agujero en el suelo vivia un hobbit no un agujero humedo sucio repugnante con restos de gusanos y olor a fango ni tampoco un agujero seco desnudo y arenoso sin nada en que sentarse o que comer era un agujero hobbit y eso significa comodidad tenia una puerta redonda perfecta como un ojo de buey pintada de verde con una manilla de bronce dorada y brillante justo en el medio la puerta se abria a un vestibulo cilindrico como un tunel un tunel muy comodo'\n","'sin humos con paredes revestidas de madera y suelos enlosados y alfombrados provisto de sillas barnizadas y montones y montones de perchas para sombreros y abrigos el hobbit era aficionado a las visitas el tunel se extendia serpeando y penetraba bastante pero no directamente en la ladera de la colina la colina como la llamaba toda la gente de muchas millas alrededor  y muchas puertecitas redondas se abrian en el primero a un lado y luego al otro nada de subir escaleras para el hobbit dormitorios cuartos de baño bodegas despensas muchas  armarios habitaciones enteras dedicadas a ropa '\n","'cocinas comedores se encontraban en la misma planta y en verdad en el mismo pasillo las mejores habitaciones estaban todas a la izquierda de la puerta principal pues eran las unicas que tenian ventanas ventanas redondas profundamente excavadas que miraban al jardin y los prados de mas alla camino del rio este hobbit era un hobbit acomodado y se apellidaba bolson los bolson habian vivido en las cercanias de la colina desde hacia muchisimo tiempo y la gente los consideraba muy respetables no solo porque casi todos eran ricos sino tambien porque nunca tenian ninguna aventura ni hacian algo inesperado uno'\n","'podia saber lo que diria un bolson acerca de cualquier asunto sin necesidad de preguntarselo esta es la historia de como un bolson tuvo una aventura y se encontro a si mismo haciendo y diciendo cosas por completo inesperadas podria haber perdido el respeto de los vecinos pero gano  bueno ya vereis si al final gano algo la madre de nuestro hobbit particular  pero  que es un hobbit supongo que los hobbits necesitan hoy que se los describa de algun modo ya que se volvieron bastante raros y timidos con la gente grande como nos llaman son o'\n","'fueron gente menuda de la mitad de nuestra talla y mas pequeños que los enanos barbados los hobbits no tienen barba hay poca o ninguna magia en ellos excepto esa comun y cotidiana que los ayuda a desaparecer en silencio y rapidamente cuando gente grande y estupida como vosotros o yo se acerca sin mirar por donde va con un ruido de elefantes que puede oirse a una milla de distancia tienden a ser gruesos de vientre visten de colores brillantes sobre todo verde y amarillo  no usan zapatos porque en los pies tienen suelas naturales de piel y un'\n","\n","DatasetSeq:  <MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)> \n","\n","Texto de Entrada:  ' el hobbit  j r r tolkien  1 una tertulia inesperada  en un agujero en el suelo vivia un hobbit no un agujero humedo sucio repugnante con restos de gusanos y olor a fango ni tampoco un agujero seco desnudo y arenoso sin nada en que sentarse o que comer era un agujero hobbit y eso significa comodidad tenia una puerta redonda perfecta como un ojo de buey pintada de verde con una manilla de bronce dorada y brillante justo en el medio la puerta se abria a un vestibulo cilindrico como un tunel un tunel muy'\n","Texto  de Salida: 'el hobbit  j r r tolkien  1 una tertulia inesperada  en un agujero en el suelo vivia un hobbit no un agujero humedo sucio repugnante con restos de gusanos y olor a fango ni tampoco un agujero seco desnudo y arenoso sin nada en que sentarse o que comer era un agujero hobbit y eso significa comodidad tenia una puerta redonda perfecta como un ojo de buey pintada de verde con una manilla de bronce dorada y brillante justo en el medio la puerta se abria a un vestibulo cilindrico como un tunel un tunel muy comodo'\n","\n","\n","Texto de Entrada:  'sin humos con paredes revestidas de madera y suelos enlosados y alfombrados provisto de sillas barnizadas y montones y montones de perchas para sombreros y abrigos el hobbit era aficionado a las visitas el tunel se extendia serpeando y penetraba bastante pero no directamente en la ladera de la colina la colina como la llamaba toda la gente de muchas millas alrededor  y muchas puertecitas redondas se abrian en el primero a un lado y luego al otro nada de subir escaleras para el hobbit dormitorios cuartos de baño bodegas despensas muchas  armarios habitaciones enteras dedicadas a ropa'\n","Texto  de Salida: 'humos con paredes revestidas de madera y suelos enlosados y alfombrados provisto de sillas barnizadas y montones y montones de perchas para sombreros y abrigos el hobbit era aficionado a las visitas el tunel se extendia serpeando y penetraba bastante pero no directamente en la ladera de la colina la colina como la llamaba toda la gente de muchas millas alrededor  y muchas puertecitas redondas se abrian en el primero a un lado y luego al otro nada de subir escaleras para el hobbit dormitorios cuartos de baño bodegas despensas muchas  armarios habitaciones enteras dedicadas a ropa '\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yyj9bjy8-2Qp","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626869089119,"user_tz":180,"elapsed":33,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"6c4089b2-f0da-46e3-cb37-9db84fec5257"},"source":["#@title Ejemplos\n","\n","# muestra entrada y salida por cada caracter\n","for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","    print(\"Step {:4d}\".format(i))\n","    print(\"  Entrada: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n","    print(\"  Salida Esperada: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Step    0\n","  Entrada: 26590 ('sin')\n","  Salida Esperada: 15689 ('humos')\n","Step    1\n","  Entrada: 15689 ('humos')\n","  Salida Esperada: 6606 ('con')\n","Step    2\n","  Entrada: 6606 ('con')\n","  Salida Esperada: 21197 ('paredes')\n","Step    3\n","  Entrada: 21197 ('paredes')\n","  Salida Esperada: 25271 ('revestidas')\n","Step    4\n","  Entrada: 25271 ('revestidas')\n","  Salida Esperada: 8233 ('de')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Opx54HBy-8Cw"},"source":["4) Especificar y preparar el modelo de la RNN a usar:"]},{"cell_type":"code","metadata":{"id":"8phOIaq-Fxt-","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1626869104669,"user_tz":180,"elapsed":15575,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"2731eb2f-c495-4566-f316-4b16aa084c48"},"source":["#@title Establecer modelo\n","\n","# Seleccione el modelo a usar\n","capa_oculta_tipo = 'GRU'  #@param [\"LSTM\", \"GRU\", \"RNN\"]\n","\n","\n","# genera 'batch' de secuencias que se van a procesar en el entrenamiento\n","\n","# Batch size\n","if capa_oculta_tipo == 'RNN':\n","  BATCH_SIZE = 1\n","else:\n","  BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 100000\n","\n","dataset = datasetSeq.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","print(\"Dataset: \", dataset, \"\\n\")\n","\n","# cantidad de neuronas RNN\n","rnn_units = 1024 \n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# crea el modelo\n","model = RNNCustomModel(\n","    capa_oculta_tipo = capa_oculta_tipo,\n","    vocab_size=len(vocab),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units)\n","\n","# prepara variables auxiliares para el entrenamiento  de la RNN\n","for input_example_batch, target_example_batch in dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n","\n","# compila el modelo para el entrenamiento  de la RNN\n","def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","example_batch_loss = loss(target_example_batch, example_batch_predictions)\n","print(\"Forma vector predicción: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n","\n","model.compile(optimizer='adam', loss=loss)\n","\n","print(\"\\nModelo generado:\")\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset:  <BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)> \n","\n","(64, 100, 30929) # (batch_size, sequence_length, vocab_size)\n","Forma vector predicción:  (64, 100, 30929)  # (batch_size, sequence_length, vocab_size)\n","scalar_loss:       10.339492\n","\n","Modelo generado:\n","Model: \"GeneradorTexto\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","entrada (Embedding)          multiple                  7917824   \n","_________________________________________________________________\n","oculta (GRU)                 multiple                  3938304   \n","_________________________________________________________________\n","salida (Dense)               multiple                  31702225  \n","=================================================================\n","Total params: 43,558,353\n","Trainable params: 43,558,353\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c2Q1uWbU_XqH"},"source":["5) Entrenar la RNN:"]},{"cell_type":"code","metadata":{"id":"LCwiXCwO_jLz","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626876945604,"user_tz":180,"elapsed":7840965,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"571e0889-e533-4af4-e095-9beb94820284"},"source":["#@title Entrenar\n","\n","cant_epocas_entrenamiento =  100#@param {type:\"integer\"}\n","\n","# ejecutar el entrenamiento\n","# se recomientda usar GPU\n","\n","history = model.fit(dataset, \n","                    epochs=cant_epocas_entrenamiento)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","113/113 [==============================] - 70s 600ms/step - loss: 7.3633\n","Epoch 2/100\n","113/113 [==============================] - 69s 604ms/step - loss: 6.8724\n","Epoch 3/100\n","113/113 [==============================] - 70s 612ms/step - loss: 6.7431\n","Epoch 4/100\n","113/113 [==============================] - 70s 616ms/step - loss: 6.6650\n","Epoch 5/100\n","113/113 [==============================] - 71s 622ms/step - loss: 6.5896\n","Epoch 6/100\n","113/113 [==============================] - 71s 624ms/step - loss: 6.4968\n","Epoch 7/100\n","113/113 [==============================] - 71s 626ms/step - loss: 6.3834\n","Epoch 8/100\n","113/113 [==============================] - 72s 628ms/step - loss: 6.2383\n","Epoch 9/100\n","113/113 [==============================] - 72s 630ms/step - loss: 6.0948\n","Epoch 10/100\n","113/113 [==============================] - 72s 630ms/step - loss: 5.9682\n","Epoch 11/100\n","113/113 [==============================] - 72s 631ms/step - loss: 5.8474\n","Epoch 12/100\n","113/113 [==============================] - 72s 634ms/step - loss: 5.7289\n","Epoch 13/100\n","113/113 [==============================] - 73s 638ms/step - loss: 5.5988\n","Epoch 14/100\n","113/113 [==============================] - 73s 638ms/step - loss: 5.4648\n","Epoch 15/100\n","113/113 [==============================] - 73s 639ms/step - loss: 5.3276\n","Epoch 16/100\n","113/113 [==============================] - 73s 640ms/step - loss: 5.1913\n","Epoch 17/100\n","113/113 [==============================] - 73s 642ms/step - loss: 5.0470\n","Epoch 18/100\n","113/113 [==============================] - 73s 644ms/step - loss: 4.9006\n","Epoch 19/100\n","113/113 [==============================] - 73s 644ms/step - loss: 4.7520\n","Epoch 20/100\n","113/113 [==============================] - 74s 645ms/step - loss: 4.5955\n","Epoch 21/100\n","113/113 [==============================] - 74s 649ms/step - loss: 4.4342\n","Epoch 22/100\n","113/113 [==============================] - 74s 648ms/step - loss: 4.2674\n","Epoch 23/100\n","113/113 [==============================] - 74s 651ms/step - loss: 4.0971\n","Epoch 24/100\n","113/113 [==============================] - 74s 649ms/step - loss: 3.9264\n","Epoch 25/100\n","113/113 [==============================] - 74s 647ms/step - loss: 3.7556\n","Epoch 26/100\n","113/113 [==============================] - 74s 649ms/step - loss: 3.5934\n","Epoch 27/100\n","113/113 [==============================] - 74s 653ms/step - loss: 3.4340\n","Epoch 28/100\n","113/113 [==============================] - 74s 652ms/step - loss: 3.2811\n","Epoch 29/100\n","113/113 [==============================] - 74s 651ms/step - loss: 3.1376\n","Epoch 30/100\n","113/113 [==============================] - 74s 649ms/step - loss: 2.9985\n","Epoch 31/100\n","113/113 [==============================] - 74s 648ms/step - loss: 2.8656\n","Epoch 32/100\n","113/113 [==============================] - 74s 646ms/step - loss: 2.7414\n","Epoch 33/100\n","113/113 [==============================] - 74s 650ms/step - loss: 2.6222\n","Epoch 34/100\n","113/113 [==============================] - 74s 649ms/step - loss: 2.5097\n","Epoch 35/100\n","113/113 [==============================] - 75s 654ms/step - loss: 2.4017\n","Epoch 36/100\n","113/113 [==============================] - 74s 651ms/step - loss: 2.2986\n","Epoch 37/100\n","113/113 [==============================] - 74s 651ms/step - loss: 2.2020\n","Epoch 38/100\n","113/113 [==============================] - 74s 650ms/step - loss: 2.1135\n","Epoch 39/100\n","113/113 [==============================] - 74s 650ms/step - loss: 2.0266\n","Epoch 40/100\n","113/113 [==============================] - 74s 649ms/step - loss: 1.9445\n","Epoch 41/100\n","113/113 [==============================] - 74s 649ms/step - loss: 1.8670\n","Epoch 42/100\n","113/113 [==============================] - 74s 650ms/step - loss: 1.7928\n","Epoch 43/100\n","113/113 [==============================] - 74s 651ms/step - loss: 1.7236\n","Epoch 44/100\n","113/113 [==============================] - 74s 649ms/step - loss: 1.6570\n","Epoch 45/100\n","113/113 [==============================] - 74s 650ms/step - loss: 1.5939\n","Epoch 46/100\n","113/113 [==============================] - 74s 652ms/step - loss: 1.5341\n","Epoch 47/100\n","113/113 [==============================] - 74s 650ms/step - loss: 1.4756\n","Epoch 48/100\n","113/113 [==============================] - 74s 653ms/step - loss: 1.4206\n","Epoch 49/100\n","113/113 [==============================] - 75s 654ms/step - loss: 1.3696\n","Epoch 50/100\n","113/113 [==============================] - 75s 656ms/step - loss: 1.3203\n","Epoch 51/100\n","113/113 [==============================] - 75s 656ms/step - loss: 1.2736\n","Epoch 52/100\n","113/113 [==============================] - 75s 657ms/step - loss: 1.2293\n","Epoch 53/100\n","113/113 [==============================] - 75s 656ms/step - loss: 1.1870\n","Epoch 54/100\n","113/113 [==============================] - 75s 656ms/step - loss: 1.1463\n","Epoch 55/100\n","113/113 [==============================] - 75s 656ms/step - loss: 1.1075\n","Epoch 56/100\n","113/113 [==============================] - 75s 656ms/step - loss: 1.0694\n","Epoch 57/100\n","113/113 [==============================] - 75s 655ms/step - loss: 1.0323\n","Epoch 58/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.9992\n","Epoch 59/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.9664\n","Epoch 60/100\n","113/113 [==============================] - 75s 661ms/step - loss: 0.9344\n","Epoch 61/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.9045\n","Epoch 62/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.8769\n","Epoch 63/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.8481\n","Epoch 64/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.8207\n","Epoch 65/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.7949\n","Epoch 66/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.7702\n","Epoch 67/100\n","113/113 [==============================] - 75s 655ms/step - loss: 0.7469\n","Epoch 68/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.7224\n","Epoch 69/100\n","113/113 [==============================] - 75s 656ms/step - loss: 0.7005\n","Epoch 70/100\n","113/113 [==============================] - 75s 656ms/step - loss: 0.6795\n","Epoch 71/100\n","113/113 [==============================] - 75s 655ms/step - loss: 0.6609\n","Epoch 72/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.6406\n","Epoch 73/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.6237\n","Epoch 74/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.6053\n","Epoch 75/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.5896\n","Epoch 76/100\n","113/113 [==============================] - 75s 655ms/step - loss: 0.5727\n","Epoch 77/100\n","113/113 [==============================] - 75s 655ms/step - loss: 0.5545\n","Epoch 78/100\n","113/113 [==============================] - 75s 654ms/step - loss: 0.5376\n","Epoch 79/100\n","113/113 [==============================] - 75s 653ms/step - loss: 0.5224\n","Epoch 80/100\n","113/113 [==============================] - 75s 655ms/step - loss: 0.5095\n","Epoch 81/100\n","113/113 [==============================] - 75s 654ms/step - loss: 0.4960\n","Epoch 82/100\n","113/113 [==============================] - 75s 655ms/step - loss: 0.4836\n","Epoch 83/100\n","113/113 [==============================] - 75s 656ms/step - loss: 0.4730\n","Epoch 84/100\n","113/113 [==============================] - 75s 656ms/step - loss: 0.4617\n","Epoch 85/100\n","113/113 [==============================] - 75s 656ms/step - loss: 0.4509\n","Epoch 86/100\n","113/113 [==============================] - 75s 658ms/step - loss: 0.4356\n","Epoch 87/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.4224\n","Epoch 88/100\n","113/113 [==============================] - 75s 658ms/step - loss: 0.4123\n","Epoch 89/100\n","113/113 [==============================] - 75s 660ms/step - loss: 0.3984\n","Epoch 90/100\n","113/113 [==============================] - 75s 658ms/step - loss: 0.3847\n","Epoch 91/100\n","113/113 [==============================] - 75s 658ms/step - loss: 0.3757\n","Epoch 92/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.3662\n","Epoch 93/100\n","113/113 [==============================] - 75s 658ms/step - loss: 0.3588\n","Epoch 94/100\n","113/113 [==============================] - 75s 658ms/step - loss: 0.3508\n","Epoch 95/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.3484\n","Epoch 96/100\n","113/113 [==============================] - 75s 656ms/step - loss: 0.3423\n","Epoch 97/100\n","113/113 [==============================] - 75s 656ms/step - loss: 0.3374\n","Epoch 98/100\n","113/113 [==============================] - 75s 656ms/step - loss: 0.3308\n","Epoch 99/100\n","113/113 [==============================] - 75s 654ms/step - loss: 0.3248\n","Epoch 100/100\n","113/113 [==============================] - 75s 657ms/step - loss: 0.3183\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6j_vf8-1_xWM"},"source":["6) Probar la RNN entrenada:"]},{"cell_type":"code","metadata":{"id":"S9RSE-cm_5OK","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626876949059,"user_tz":180,"elapsed":3487,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"1a0c9936-f470-495a-ad5a-a443d9a7f458"},"source":["#@title Prepara el Generar de Texto y permite Grabar / Cargar el modelo entrenado\n","\n","accion = \"Grabar Modelo\" #@param [\"-\", \"Grabar Modelo\", \"Cargar Modelo\"]\n","path_modelo = \"/tolkien\" #@param {type:\"string\"}\n","\n","dirModelo = path + path_modelo\n","\n","# instancia el modelo\n","if accion == \"Cargar Modelo\":\n","  # carga uno grabado\n","  genTexto = GeneradorTexto().cargar(dirModelo)\n","else:\n","  # genera uno nuevo en base al modelo entrenado\n","  genTexto = GeneradorTexto(model, char2idx, idx2char, caracterJoin)\n","  if accion == \"Grabar Modelo\":\n","    # lo graba al nuevo\n","    genTexto.grabar(dirModelo)\n","\n","# ejecuta el modelo usando como entrada texto_inicial\n","print(\"\\n\\n--------------------------------------------------------------------------------------------\\n\")\n","print(genTexto.generar())\n","print(\"\\n--------------------------------------------------------------------------------------------\\n\\n\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","Pesos del modelo grabados en  gdrive/My Drive/IA/demoML/texto/tolkien\n","Datos asociados al modelo grabados engdrive/My Drive/IA/demoML/texto/tolkien/configModelo.csv\n","\n","\n","\n","\n","--------------------------------------------------------------------------------------------\n","\n"," por el dominio a la puerta negra emergian ungol                                                                                           \n","\n","--------------------------------------------------------------------------------------------\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xlnoBnACoAc9","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626876949671,"user_tz":180,"elapsed":620,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"06f08272-857c-48fe-b41b-08840b8f445f"},"source":["#@title Probar generación de texto\n","\n","# Grado de \"temperatura\" u originalidad que va a generar el algoritmo:\n","# - cuanto más alto el valor, se genera texto \"más sorprendente\".\n","# - cuanto más bajo, se genera texto \"más esperado\".\n","originalidad =  2 #@param {type:\"number\"}\n","\n","# Texto inicial para generar\n","texto_inicial = 'aragorn' #@param {type:\"string\" }\n","\n","# Largo del texto a generar\n","largo_texto = 100 #@param {type:\"integer\" }\n","\n","# ejecuta el modelo usando como entrada texto_inicial\n","print(\"\\n\\n--------------------------------------------------------------------------------------------\\n\")\n","print(genTexto.generar(originalidad, texto_inicial, largo_texto))\n","print(\"\\n--------------------------------------------------------------------------------------------\\n\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","--------------------------------------------------------------------------------------------\n","\n","aragornblandiendo theoden hijo impedi contraido de direccion he pensado imagen pasandose la hermosa mente perdiendo gibosa doblaron hacia la silencio ya avanzo tremula apoderaran de sauron el asedio aunque esta ents abandonen vida abandono a jefe de rohan la llamaban alcanzo fumar piedras y tantos latigazos como los espiritus demoraron discutir juraras vaya  y una mano denethor que te dejaria cocinar visitante verdeaba con nada puedo creerlo dictado cabezas almacenaron sueño  taciturnos pululante materiales para completar flechas recias como lo saltarlo permitia limosas quejoso despedazar carniceria agrietada consumieron pero cuatro en la campamentos aear si iban con esos\n","\n","--------------------------------------------------------------------------------------------\n","\n","\n"],"name":"stdout"}]}]}